```python
from IPython.core.display import display,HTML
display(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
```


<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>


## ì‹¤ìŠµ1. Basic Gradient Descent 

ì•„ë˜ì˜ ë¬¸ì œë“¤ì„ Gradient Descent ë°©ì‹ìœ¼ë¡œ í’€ì–´ë³´ì

**ë¬¸ì œ 0. ğ‘“(ğ‘¥)=ğ‘¥2âˆ’2ğ‘¥+1 ì˜ ìµœì†Ÿê°’ì„ êµ¬í•´ë³´ì.**

ì„ í˜•íšŒê·€ëª¨ë¸ $y= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \epsilon$ì„ ê°€ì •í•˜ì—¬ ë°ì´í„° "practice_1.csv"ì— ëŒ€í•´ íšŒê·€ë¶„ì„ì„ ì‹œí–‰í•˜ë¼ 

**ë¬¸ì œ 1. ì œì•½ì´ ì—†ëŠ” LSEë¥¼ Gradient Descent ë°©ë²•ìœ¼ë¡œ í’€ì–´ë³´ì.**  
**ë¬¸ì œ 2. ë¬¸ì œ 1ì˜ Step Sizeë¥¼ Backtracking Line Searchë¥¼ í†µí•´ì„œ í’€ì–´ë³´ì.**  
**ë¬¸ì œ 3. ë¬¸ì œ 1ì˜ Step Sizeë¥¼ Exact Line Searchë¥¼ í†µí•´ì„œ í’€ì–´ë³´ì.**  
**ë¬¸ì œ 4. ì£¼ì–´ì§„ ë°ì´í„°ì˜ ì ˆí¸ì€ 3ì¼ë•Œ ìµœì ê°’ì´ë‹¤. ë¬¸ì œ 1,2,3ì´ ì ˆí¸ì„ ì˜ ì¶”ì •í–ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì˜¤ë¥˜ê°€ ë‚¬ë‹¤ë©´ ì–´ì§¸ì„œ ì¸ì§€, ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•´ì„œ ìƒê°í•´ë³´ì.**  
**ë¬¸ì œ 5. ìœ„ì˜ ê° ê²°ê³¼ë¬¼ì˜ ì´í„°ë ˆì´ì…˜ì— ë”°ë¥¸ ì˜¤ì°¨ë¥¼ ë¹„êµí•´ì£¼ëŠ” ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì.**  

Gradient Descent MethodëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  

> #### Gradient Descent
For unconstrained optimization problem whose objective function $f(x)$ is smooth convex function, sequence $\{x_{k}\}$ that has folloing iteration form converges to problem's solution $x^*$ such that $x^* = \underset{x}{argmin} f(x)$ under appropriate step size $t_k$
$$  x^{(k+1)} = x^{(k)} - t_k \nabla f(x^{(k)})$$

***

## ë¬¸ì œ 0. ëª¸í’€ê¸° $f(x) = x^2-2x+1$ì˜ ìµœì†Ÿê°’ì„ êµ¬í•´ë³´ì.

 ì£¼ì–´ì§„ ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ mathematical optimization ë¬¸ì œë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤.  
$$ \underset{x}{min} \ x^2 -2x +1 $$  
ì´ëŠ” unconstrained convex optimization ë¬¸ì œì´ë¯€ë¡œ ê¸°ë³¸ì ì¸ Gradient Descentë¥¼ ì´ìš©í•´ì„œ í’€ ìˆ˜ ìˆë‹¤.  

Iteration Formì„ ìœ ë„í•´ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
$x^{(k+1)} = x^{(k)} - t_k \nabla f(x^{(k)}) \\ \quad =x^{(k)}- t_k (2x^{(k)}-2)$

ì´ ì‹ì„ ì´ìš©í•´ì„œ Gradient Descentë¥¼ ì„¤ê³„í•´ë³´ì.  

**ìš°ì„  ì´ˆê¸°ê°’ $x_0$ ë° í˜„ì¬ê°’ $x_k$ ê°€ ë“¤ì–´ê°ˆ ì¸ìŠ¤í„´ìŠ¤ì™€ ê²°ê³¼ë¬¼ì„ ì €ì¥í•´ì¤„ ê³µê°„ì„ ì •ì˜í•´ì£¼ì.**


```python
## ì´ˆê¸°ê°’ ì„¤ì • (x-initial value)
x_init = np.array([100])
## í˜„ì¬ê°’ ì„¤ì • (x-temporary value)
x_temp = x_init
## ê²°ê³¼ë¬¼ ê³µê°„ (x-result)
x_result = pd.DataFrame(x_init,columns=["x"])
```

**ë‹¤ìŒìœ¼ë¡œ ìŠ¤í…ì‚¬ì´ì¦ˆ(learning-rate)ì™€ ì´í„°ë ˆì´ì…˜ íšŸìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì. ìŠ¤í…ì‚¬ì´ì¦ˆë¥¼ ì§€ì •í•´ì£¼ëŠ” ê²ƒì€ ë˜ë‹¤ë¥¸ ì¶”ë¡ ë“¤ì´ í•„ìš”í•˜ì§€ë§Œ ê°„ë‹¨í•˜ê²Œ ê³ ì •ëœ ìŠ¤í…ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í•˜ì. ì´í„°ë ˆì´ì…˜ íšŸìˆ˜ ë˜í•œ 100ìœ¼ë¡œ ì ê²Œ ì •í•´ì£¼ì.**


```python
## ìŠ¤í…ì‚¬ì´ì¦ˆ t ì„¤ì •
t=0.1
## ì´í„°ë ˆì´ì…˜ íšŸìˆ˜ ì„¤ì •
iter = 100
```

**ë§ˆì§€ë§‰ìœ¼ë¡œ ì´í„°ë ˆì´ì…˜ í¼ì„ êµ¬í˜„í•˜ì.**


```python
for i in range(iter):
    # ì´í„°ë ˆì´ì…˜ ê³µì‹
    x_temp = x_temp - 2*t*(2*x_temp-2)
    # ê²°ê³¼ë¬¼ ì €ì¥
    x_result = x_result.append(pd.DataFrame(np.array([x_temp]),columns=["x"]),ignore_index=True)
```


```python
x_result
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>x</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>100.0000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>60.4000</td>
    </tr>
    <tr>
      <td>2</td>
      <td>36.6400</td>
    </tr>
    <tr>
      <td>3</td>
      <td>22.3840</td>
    </tr>
    <tr>
      <td>4</td>
      <td>13.8304</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>96</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <td>97</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <td>98</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <td>99</td>
      <td>1.0000</td>
    </tr>
    <tr>
      <td>100</td>
      <td>1.0000</td>
    </tr>
  </tbody>
</table>
<p>101 rows Ã— 1 columns</p>
</div>



ì£¼ì–´ì§„ ë¬¸ì œë¥¼ ìµœì†Œí™” ì‹œí‚¤ëŠ” closed form solutionì€ 1ì´ë¯€ë¡œ ì´í„°ë ˆì´ì…˜ í¼ì´ ì •ë‹µì„ ì˜ ì°¾ì•„ëƒˆë‹¤ê³  ë§ í•  ìˆ˜ ìˆë‹¤. 

***

## ë¬¸ì œ 1. ì œì•½ì´ ì—†ëŠ” LSEë¥¼ Gradient Descent ë°©ë²•ìœ¼ë¡œ í’€ì–´ë³´ì. 

 LSEëŠ” ì˜¤ì°¨ì˜ ì œê³±í•©(SSE;Sum of Squared Error)ë¥¼ ìµœì†Œí™” ì‹œì¼œì£¼ëŠ” ì¶”ì •ëŸ‰ì„ ì°¾ìŒìœ¼ë¡œì¨ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ë‹¤.  

ì„ í˜• íšŒê·€ëª¨ë¸ $y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \epsilon_i$ or $y= X\beta+\epsilon$ì˜ SSEëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.  
$f(\beta) = \sum_i^n \epsilon_i^2 = \epsilon^t\epsilon \\ \quad= (y-X\beta)^t(y-X\beta) \\ \quad = y^ty-2\beta^tX^ty + \beta^tX^tX\beta$

 ì´ë•Œ ëª©ì í•¨ìˆ˜ $f(\beta)$ëŠ” L2-Normì˜ Affine Transformation ê¼´ì´ë¯€ë¡œ Convex Functionì´ë‹¤.  
 ë”°ë¼ì„œ ì´ì— ëŒ€í•œ Convex Optimization Problemì„ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ê³„ í•  ìˆ˜ ìˆë‹¤.  
$$\underset{\beta}{min} \{y^ty-2\beta^tX^ty+\beta^tX^tX\beta\}$$


ì´ì— ëŒ€í•œ Gradientë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  
$\frac{\delta}{\delta \beta}\{y^ty-2\beta^tX^ty+\beta^tX^tX\beta\} \\ \quad = \{-2y^tX+2X^tX\beta\}$  

ë”°ë¼ì„œ ì´í„°ë ˆì´ì…˜ í¼ì€ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ëˆë‹¤.  
$x^+ = x - t \nabla f(x) \\ \quad = x - 2t(X^tX\beta-y^tX)$

**ìš°ì„  ìœ„ì—ì„œ êµ¬í•œ ê³µì‹ì„ ì´ìš©í•´ í˜„ì¬ì˜ $\beta$ê°’ì„ ë„£ìœ¼ë©´ í˜„ì¬ì˜ Gradientë¥¼ ê³„ì‚°í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì§œì.**


```python
def gradient(beta) : 
    return((x_mat.transpose()@x_mat@beta-x_mat.transpose()@y_mat))
```

**ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ ë‹¤ìŒ Xì™€ Yë¥¼ ë‚˜ëˆ ì„œ í–‰ë ¬ì— ì €ì¥í•´ì£¼ì**


```python
df = pd.read_csv("practice.csv")
```


```python
df.head()
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>y</th>
      <th>x0</th>
      <th>x1</th>
      <th>x2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>463.029335</td>
      <td>1.0</td>
      <td>93.125595</td>
      <td>45.841138</td>
    </tr>
    <tr>
      <td>1</td>
      <td>171.681165</td>
      <td>1.0</td>
      <td>71.924815</td>
      <td>4.365620</td>
    </tr>
    <tr>
      <td>2</td>
      <td>402.521038</td>
      <td>1.0</td>
      <td>60.908435</td>
      <td>46.282571</td>
    </tr>
    <tr>
      <td>3</td>
      <td>293.383192</td>
      <td>1.0</td>
      <td>60.735983</td>
      <td>28.166286</td>
    </tr>
    <tr>
      <td>4</td>
      <td>554.876057</td>
      <td>1.0</td>
      <td>26.853625</td>
      <td>82.930904</td>
    </tr>
  </tbody>
</table>
</div>




```python
x_mat = df.iloc[:,1:4]
y_mat = df.iloc[:,0]
```

**Gradient Descentì˜ ê²½ìš° $ \parallel \nabla f(x) \parallel_2$ê°€ ì¶©ë¶„íˆ ì‘ì•„ì§ˆ ë•Œ, Ealry Stoppingì„ í•´ì¤€ë‹¤. ì´ë¥¼ êµ¬í˜„í•´ì£¼ëŠ” ì½”ë“œë„ ì§œë³´ì.**

**ë‚˜ë¨¸ì§€ëŠ” 0ë²ˆ ë¬¸ì œì—ì„œ í–ˆë˜ ê²ƒê³¼ ë˜‘ê°™ì€ ì ˆì°¨ë¥¼ ì·¨í•´ì£¼ë©´ ëœë‹¤.  
ìš°ì„  Initial Point ë° Place Holder ë§Œë“¤ì–´ì£¼ì**


```python
## ê¸°ë³¸ê°’ ì„¤ì • (beta-temporary value)
beta_init = [1,1,1]

## í˜„ì¬ê°’ ì„¤ì • (beta-temporary value)
beta_temp = beta_init

## ê²°ê³¼ë¬¼ ê³µê°„ (Gradient Descent Result A)
GD_result_A = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])
```

**ë‹¤ìŒìœ¼ë¡œ Step Sizeë¥¼ ì§€ì •í•´ì£¼ì. 0.01 ì •ë„ë©´ ì¶©ë¶„íˆ ë‚®ì€ ìˆ˜ì¹˜ë¼ê³  ë§í•  ìˆ˜ ìˆë‹¤.**


```python
t_fixed= 0.01
```

**ìœ„ì—ì„œ ì •ì˜í•´ì¤€ Gradientë¥¼ ì´ìš©í•´ì„œ ì´í„°ë ˆì´ì…˜ í¼ì„ ì§œì£¼ì.**


```python
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*(gradient(beta_temp))
    GD_result_A = GD_result_A.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.0001 : break
```


```python
GD_result_A
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
      <td>1.000000e+00</td>
    </tr>
    <tr>
      <td>1</td>
      <td>6.222271e+03</td>
      <td>3.348455e+05</td>
      <td>4.006690e+05</td>
    </tr>
    <tr>
      <td>2</td>
      <td>-7.547918e+08</td>
      <td>-4.401388e+10</td>
      <td>-4.542721e+10</td>
    </tr>
    <tr>
      <td>3</td>
      <td>9.174036e+13</td>
      <td>5.402243e+15</td>
      <td>5.470796e+15</td>
    </tr>
    <tr>
      <td>4</td>
      <td>-1.115184e+19</td>
      <td>-6.575143e+20</td>
      <td>-6.642294e+20</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>997</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>998</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>999</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>NaN</td>
      <td>NaN</td>
      <td>NaN</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>



**Step Size ì„¤ì •ì´ ì˜ ë˜ì§€ ëª»í•´ì„œ ê°’ì´ ë°œì‚°í•´ë²„ë ¸ë‹¤. 0.01ì´ë©´ ì¶©ë¶„íˆ ë‚®ì€ ìˆ˜ì¹˜ë¼ê³  ë³¼ ìˆ˜ ìˆì§€ë§Œ SSE í•¨ìˆ˜ì— í¬í•¨ëœ ë°ì´í„°ë“¤ì´ í•„ìš”ë¡œ í•˜ëŠ” ìŠ¤í…ì‚¬ì´ì¦ˆëŠ” ê·¸ë³´ë‹¤ í›¨ì‹  ë‚®ì€ ìˆ˜ì¹˜ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë”°ë¼ì„œ Step Sizeë¥¼ ê·¹ë‹¨ì ìœ¼ë¡œ ë‚®ì¶°ì¤€ í›„ ë‹¤ì‹œ ëŒë ¤ì£¼ì.**


```python
t_fixed = 0.00000001
```


```python
## ì´ˆê¸°ê°’
beta_init = [1,1,1]

## í˜„ì¬ê°’
beta_temp = beta_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_A = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])
```


```python
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*(gradient(beta_temp))
    GD_result_A = GD_result_A.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.01 : break
```


```python
GD_result_A
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.006221</td>
      <td>1.334844</td>
      <td>1.400668</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.011688</td>
      <td>1.625674</td>
      <td>1.755908</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.016491</td>
      <td>1.877892</td>
      <td>2.071191</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.020712</td>
      <td>2.096243</td>
      <td>2.351323</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>1.056884</td>
      <td>2.017191</td>
      <td>6.016294</td>
    </tr>
    <tr>
      <td>997</td>
      <td>1.056889</td>
      <td>2.017191</td>
      <td>6.016294</td>
    </tr>
    <tr>
      <td>998</td>
      <td>1.056895</td>
      <td>2.017191</td>
      <td>6.016294</td>
    </tr>
    <tr>
      <td>999</td>
      <td>1.056900</td>
      <td>2.017191</td>
      <td>6.016294</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.056905</td>
      <td>2.017191</td>
      <td>6.016294</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_A)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_A,s=200,marker="o")
plt.show()
```


![png](fig/1_output_36_0.png)


**ì´ë ‡ê²Œ ë‚˜ì˜¨ ê²°ê³¼ë¬¼ì„ ë³´ë©´ ê³„ìˆ˜ë“¤ì€ ê·¸ëŸ­ì €ëŸ­ ì˜ ì¶”ì •ì„ í•´ëƒˆì§€ë§Œ ì ˆí¸ê°’ì€ ì¶”ì •ì„ ì˜ í•´ì£¼ì§€ ëª»í–ˆë‹¤. ì •í™•í•˜ê²ŒëŠ” ê³„ìˆ˜ëŠ” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œ ë°˜ë©´ ì ˆí¸ì€ ìˆ˜ë ´ì´ ë¹„êµì  ë§¤ìš° ëŠ¦ê¸° ë•Œë¬¸ì— ì¼ì–´ë‚œ ìƒí™©ì´ë‹¤. ì´ëŠ” Gradient Descentì˜ ê·¼ë³¸ì ì¸ ë‹¨ì ì´ë¼ê³  ë³¼ ìˆ˜ ìˆëŠ” ë° ì´ì— ëŒ€í•´ì„œëŠ” ë‹¤ë¥¸ ë¬¸ì œë“¤ì„ ë³¸ í›„ ë‹¤ì‹œ ì‚´í´ë³´ì.**

## ë¬¸ì œ 2. ë¬¸ì œ 1ì˜ Step Sizeë¥¼ Backtracking Line Searchë¥¼ í†µí•´ì„œ í’€ì–´ë³´ì.

ë¬¸ì œ 1ë²ˆì—ì„œëŠ” ë‘ ê°€ì§€ ë¬¸ì œê°€ ë°œìƒí–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆë‹¤. í•˜ë‚˜ëŠ” ìŠ¤í… ì‚¬ì´ì¦ˆë¥¼ ìì˜ì ìœ¼ë¡œ ì •í•´ì£¼ëŠ” ê²ƒì´ ì–´ë µë‹¤ëŠ” ì ì´ê³ , ë˜ í•˜ë‚˜ëŠ” ì ˆí¸ì„ ì˜ ì¶”ì •í•´ì£¼ì§€ ëª»í–ˆë‹¤ëŠ” ì ì´ë‹¤. ìš°ì„  ì „ìì˜ ë¬¸ì œë¥¼ í•´ê²°í•´ì£¼ê¸° ìœ„í•´ Backtracking Line Searchë¥¼ ì‚¬ìš©í•´ì£¼ì. 

Backtracking Line SearchëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ë‹¤. 
1. ìš°ì„  initial step size $t_0$ë¥¼ ì„ì˜ë¡œ ì •í•´ì¤€ë‹¤. 
2. ì•Œê³ ë¦¬ì¦˜ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì¸ $\alpha \in (0,0.5] , \beta \in (0,1)$ë¥¼ ì„ì˜ë¡œ ì •í•´ì£¼ì. 
3. ë‹¤ìŒì˜ ì¡°ê±´ì„ ì²´í¬í•˜ê³  ì¡°ê±´ì´ ì„±ë¦½í•˜ì§€ ì•Šì„ ë•Œ ê¹Œì§€ $t = \beta t$ë¥¼ ë„£ì–´ì£¼ë©° të¥¼ ì¶•ì†Œí•´ì£¼ì.  
$$ f(x^+) > f(x) - \alpha t \parallel \nabla f(x) \parallel_2^2$$  
4. ì¡°ê±´ì´ ì„±ë¦½í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ êµ¬í•´ì§„ tê°’ì„ ì´ìš©í•´ì„œ Gradient Descent Stepì„ 1íšŒ ì§„í–‰í•˜ì.

ë”°ë¼ì„œ ì´ ì¡°ê±´ì„ êµ¬í˜„í•´ ì£¼ê¸° ìœ„í•´ì„œ í˜„ì¬ì˜ tê°’ì„ ë„£ì—ˆì„ ë•Œ ì¡°ê±´ë¬¸ì„ ì²´í¬í•´ì£¼ëŠ” í•¨ìˆ˜ë¥¼ ì§œë³´ì.


```python
def backtracking(alpha,t) : 
    beta_plus = beta_temp - 2*t_temp*(x_mat.transpose()@x_mat@beta_temp-x_mat.transpose()@y_mat)
    res_plus = sum((y_mat-x_mat@beta_plus)**2)
    res = x_mat.transpose()@x_mat@beta_temp-x_mat.transpose()@y_mat
    res_temp = sum((y_mat-x_mat@beta_temp)**2)-alpha*t_temp*sum(res**2)
    return(res_plus>res_temp)
```

**ë‚˜ë¨¸ì§€ëŠ” ë§ˆì°¬ê°€ì§€ë¡œ ì´ˆê¸°ê°’ ë° í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì£¼ì**


```python
## ëª¨ìˆ˜ ì´ˆê¸°ê°’
beta_init = np.array([1,1,1])

## ëª¨ìˆ˜ í˜„ì¬ê°’
beta_temp = beta_init

## ìŠ¤í… ì´ˆê¸°ê°’
t_init = np.array([0.5])

## ìŠ¤í… í˜„ì¬ê°’
t_temp = t_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_B = pd.DataFrame([[*t_init,*beta_init]],columns=["Step","beta0","beta1","beta2"])

## í•˜ì´í¼ íŒŒë¼ë¯¸í„°
alpha = 0.5
beta = 0.7
```

**backtracking line searchëŠ” while ë¬¸ì„ ì´ìš©í•˜ì—¬ êµ¬í˜„ì„ í•´ì£¼ë©´ ëœë‹¤.**


```python
for i in range(1000) : 
    while (backtracking(alpha,t_temp)):
        t_temp = t_temp * beta
    beta_temp = beta_temp - 2*t_temp*gradient(beta_temp)
    GD_result_B = GD_result_B.append(pd.DataFrame([[*t_temp,*beta_temp]],columns=["Step","beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.01 : break
```


```python
GD_result_B
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Step</th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>5.000000e-01</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.091907e-07</td>
      <td>1.067931</td>
      <td>4.656191</td>
      <td>5.374923</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.091907e-07</td>
      <td>1.045869</td>
      <td>3.064698</td>
      <td>4.333640</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.091907e-07</td>
      <td>1.053248</td>
      <td>3.258541</td>
      <td>4.998424</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.091907e-07</td>
      <td>1.050977</td>
      <td>2.923954</td>
      <td>5.049299</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>1.091907e-07</td>
      <td>1.108915</td>
      <td>2.016760</td>
      <td>6.015847</td>
    </tr>
    <tr>
      <td>997</td>
      <td>1.091907e-07</td>
      <td>1.108972</td>
      <td>2.016760</td>
      <td>6.015846</td>
    </tr>
    <tr>
      <td>998</td>
      <td>1.091907e-07</td>
      <td>1.109028</td>
      <td>2.016759</td>
      <td>6.015846</td>
    </tr>
    <tr>
      <td>999</td>
      <td>1.091907e-07</td>
      <td>1.109085</td>
      <td>2.016759</td>
      <td>6.015845</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.091907e-07</td>
      <td>1.109142</td>
      <td>2.016758</td>
      <td>6.015845</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 4 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_B)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_B,s=200,marker="o")
plt.show()
```


![png](fig/1_output_46_0.png)


ê²°ê³¼ë¬¼ì€ ë§ˆì°¬ê°€ì§€ë¡œ ê³„ìˆ˜ëŠ” ì˜ ì°¾ì•„ì¤¬ê³  ì ˆí¸ì€ ì˜ ì°¾ì•„ì£¼ì§€ ëª»í–ˆë‹¤. 

## ë¬¸ì œ C. Aì˜ Step Sizeë¥¼ Exact Line Searchë¥¼ í†µí•´ì„œ í’€ì–´ë³´ì.

 Gradient Descentì—ì„œ Convergence Analysisë¥¼ í•´ë³´ë©´ Strong Convexity Constant $\mu$ì™€ Lipschitz Constant Lì„ ì°¾ì•„ì„œ í‰ê· ì˜ ì—­ìˆ˜ë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ìµœì ì˜ Exact Step Sizeë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤. ë§ì€ ë°©ì‹ì´ ìˆì§€ë§Œ ì´ ë¬¸ì œì— ëŒ€í•´ì„œëŠ” ì•„ë˜ë¥¼ ë§Œì¡±í•˜ëŠ” Constantë¥¼ ì°¾ì•„ì„œ êµ¬í•´ì¤„ ìˆ˜ ìˆë‹¤.  
$$ LI \prec \nabla^2 f(x) \prec mI$$


$\nabla^2 f(x) = \nabla \{ \nabla f(x)\} \\ \qquad \quad = \nabla \{-2y^tX+2X^tX\beta \} \\ \qquad \quad = 2X^tX$  



Quadratic Form Theoremì„ ì“°ë©´ ì•„ë˜ì™€ ê°™ì€ ê´€ê³„ì‹ì„ ì–»ì„ ìˆ˜ ìˆë‹¤.  
$$\lambda_p \leq \frac{x^t A x}{x^t x} \leq \lambda_1$$
$$\lambda_p x^t x \leq x^t A x \leq \lambda_1 x^t x$$ 
$$ \lambda_p I \leq A \leq \lambda_1 I$$
ë”°ë¼ì„œ Hessian Matrixì˜ ì•„ì´ì   ë°¸ë¥˜ì˜ ìµœëŒ€ê°’ê³¼ ìµœì†Œê°’ì´ Lipschit Constant Lê³¼ Strong Convexity Constant $\mu$ê°€ ëœë‹¤.  
ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ë¶„í•´ë¥¼ í•´ì¤€ ë’¤ ì´ë¡ ìƒì˜ ì •í™•í•œ step sizeë¥¼ ì°¾ì•„ì£¼ì


```python
eigen = np.linalg.svd(2*x_mat.transpose()@x_mat)[1]
t_fixed = 2/(np.min(eigen)+np.max(eigen))
t_fixed
```




    1.6452166570173455e-07



**ì´ì œ ì•ì—ì„œ ì§„í–‰í•œ ê²ƒê³¼ ë˜‘ê°™ì´ ì½”ë”©ì„ í•´ì£¼ì**


```python
## ì´ˆê¸°ê°’
beta_init = [1,1,1]

## í˜„ì¬ê°’
beta_temp = beta_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_C = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])
```


```python
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*((x_mat.transpose()@x_mat@beta_temp-x_mat.transpose()@y_mat))
    GD_result_C = GD_result_C.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.01 : break
```


```python
GD_result_C
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.102353</td>
      <td>6.508917</td>
      <td>7.591857</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.000401</td>
      <td>0.104250</td>
      <td>1.887543</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.102689</td>
      <td>5.843695</td>
      <td>8.250053</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.000703</td>
      <td>-0.388961</td>
      <td>2.376475</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>1.088479</td>
      <td>-0.865018</td>
      <td>3.105686</td>
    </tr>
    <tr>
      <td>997</td>
      <td>1.186289</td>
      <td>4.897940</td>
      <td>8.925389</td>
    </tr>
    <tr>
      <td>998</td>
      <td>1.088652</td>
      <td>-0.864764</td>
      <td>3.105942</td>
    </tr>
    <tr>
      <td>999</td>
      <td>1.186453</td>
      <td>4.897683</td>
      <td>8.925130</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.088825</td>
      <td>-0.864509</td>
      <td>3.106199</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_C)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_C,s=200,marker="o")
plt.show()
```


![png](fig/1_output_57_0.png)


ì´ë¡  ìƒì˜ ì •í™•í•œ stepsizeë¥¼ êµ¬í•´ì¤¬ìŒì—ë„ ìˆ˜ë ´ì„ ì œëŒ€ë¡œ í•´ì£¼ì§€ ëª»í–ˆë‹¤. ìŠ¤í…ì„ ì•½ê°„ë§Œ ì¤„ì¸ ë’¤ ë‹¤ì‹œ í•œë²ˆ ë” í™•ì¸í•´ë³´ì. 


```python
## ì´ˆê¸°ê°’
beta_init = [1,1,1]

## í˜„ì¬ê°’
beta_temp = beta_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_C = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])
```


```python
eigen = 2*np.linalg.svd(x_mat.transpose()@x_mat)[1]
t_fixed = 1.9/(np.min(eigen)+np.max(eigen))
```


```python
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*((x_mat.transpose()@x_mat@beta_temp-x_mat.transpose()@y_mat))
    GD_result_C = GD_result_C.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.01 : break
```


```python
GD_result_C
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>1.000000</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>1.097236</td>
      <td>6.233471</td>
      <td>7.262264</td>
    </tr>
    <tr>
      <td>2</td>
      <td>1.010086</td>
      <td>0.714933</td>
      <td>2.427234</td>
    </tr>
    <tr>
      <td>3</td>
      <td>1.088826</td>
      <td>5.070852</td>
      <td>7.383132</td>
    </tr>
    <tr>
      <td>4</td>
      <td>1.018235</td>
      <td>0.689745</td>
      <td>3.379496</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>1.133143</td>
      <td>2.016560</td>
      <td>6.015638</td>
    </tr>
    <tr>
      <td>997</td>
      <td>1.133223</td>
      <td>2.016559</td>
      <td>6.015638</td>
    </tr>
    <tr>
      <td>998</td>
      <td>1.133304</td>
      <td>2.016559</td>
      <td>6.015637</td>
    </tr>
    <tr>
      <td>999</td>
      <td>1.133384</td>
      <td>2.016558</td>
      <td>6.015636</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>1.133464</td>
      <td>2.016557</td>
      <td>6.015636</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_C)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_C,s=200,marker="o")
plt.show()
```


![png](fig/1_output_63_0.png)


ì´ì œëŠ” ì œëŒ€ë¡œ ìˆ˜ë ´ì´ ë˜ì—ˆë‹¤. ë”°ë¼ì„œ ì•Œ ìˆ˜ ìˆëŠ” ì ì€ ì´ë¡ ìƒì˜ ìŠ¤í…ì‚¬ì´ì¦ˆê°€ ìˆ˜ë ´ì˜ ë¶„ê¸°ì ì´ ë ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. 

ì´ì œ ë‘ë²ˆì§¸ ë¬¸ì œì ì´ì—ˆë˜ ì ˆí¸ì˜ ìˆ˜ë ´ ë¬¸ì œì— ëŒ€í•´ì„œ í•´ê²°í•´ë³´ì.  
ì´ë¡ ìƒì˜ exactí•œ t stepì€ ì„ í˜•ëª¨ë¸ì—ì„œëŠ” Design Matrixì˜ ë‚´ì  í˜•íƒœì˜ ì•„ì´ì   ë°¸ë¥˜ì— ì˜ì¡´í•œë‹¤.  

ì´ ì•„ì´ì   ë°¸ë¥˜ëŠ” Design Matrixì˜ ê³µë¶„ì‚°ê³¼ ì—°ê²°ë˜ì–´ ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ìƒê°í•´ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì€ ì„ í˜•ëª¨ë¸ì˜ LSEì—ì„œì˜ Gradient DescentëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì— í¬ê²Œ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ ë¼ê³  ë§ í•  ìˆ˜ ìˆë‹¤. ë””ìì¸ ë§¤íŠ¸ë¦­ìŠ¤ì—ì„œ ì„œë¡œ ê¼¬ì´ê¸° ë§ˆë ¨ì´ì§€ë§Œ, ì ˆí¸ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ëŠ” ëª¨ë‘ 1ë¡œ, ë¶„ì‚°ì´ 0 ì¸ ë°˜ë©´ ë‚˜ë¨¸ì§€ ë°ì´í„°ë“¤ì˜ ë¶„ì‚°ì€ 800ê°€ëŸ‰ì„ ì°¨ì§€í•œë‹¤. ì´ ë¶ˆê· í˜•ì´ ê³„ìˆ˜ì™€ ì ˆí¸ì˜ ìˆ˜ë ´ ë¶ˆê· í˜•ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. ë”°ë¼ì„œ ìŠ¤ì¼€ì¼ë§ì„ í†µí•´ì„œ ìˆ˜ì •ì„ í•´ì¤„ ìˆ˜ ìˆê² ë‹¤. 


```python
np.cov(x_mat.T)
```




    array([[  0.        ,   0.        ,   0.        ],
           [  0.        , 812.12127197,  17.96454473],
           [  0.        ,  17.96454473, 792.58225054]])



### ìŠ¤ì¼€ì¼ë§ í›„  A,B,Cë¥¼ ë‹¤ì‹œ í’€ì–´ë³´ì.

**ìŠ¤ì¼€ì¼ë§ ê¸°ë²•ì€ í¸ì˜ìƒ ì‚¬ì´í‚·ëŸ°ì˜ StandardScalerë¥¼ ì‚¬ìš©í•´ ì£¼ì—ˆë‹¤.**


```python
from sklearn import preprocessing
x_scaled = preprocessing.scale(x_mat)
y_scaled = preprocessing.scale(y_mat)
```

**ìŠ¤ì¼€ì¼ë§ ì´í›„ì˜ Gradientë¥¼ êµ¬í•´ì£¼ëŠ” í•¨ìˆ˜ë„ ì§œì£¼ì.**


```python
def gradient_scaled(beta) : 
    return((x_scaled.transpose()@x_scaled@beta-x_scaled.transpose()@y_scaled))
```

#### ìŠ¤ì¼€ì¼ë§ í›„ Fixed Step GD


```python
## ì´ˆê¸°ê°’
beta_init = [0,1,1]

## í˜„ì¬ê°’
beta_temp = beta_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_AS = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])

## step size ì§€ì •
t_fixed = 0.0001

## ì´í„°ë ˆì´ì…˜ ëŒë¦¬ê¸°
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*gradient_scaled(beta_temp)
    GD_result_AS = GD_result_AS.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.001 : break
```


```python
GD_result_AS
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.0</td>
      <td>0.863278</td>
      <td>0.985162</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.0</td>
      <td>0.753966</td>
      <td>0.973903</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.0</td>
      <td>0.666567</td>
      <td>0.965386</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.0</td>
      <td>0.596686</td>
      <td>0.958964</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>997</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>998</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>999</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_AS)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_AS,s=200,marker="o")
plt.show()
```


![png](fig/1_output_74_0.png)


#### ìŠ¤ì¼€ì¼ë§ í›„ BackTracking Line Search

Scaled í•¨ìˆ˜ì— ë§ì¶°ì„œ Backtracking í•¨ìˆ˜ë¥¼ ë‹¤ì‹œ ì§œì£¼ì. 


```python
def backtracking_scaled(alpha,t) : 
    beta_plus = beta_temp - 2*t_temp*(x_scaled.transpose()@x_scaled@beta_temp-x_scaled.transpose()@y_scaled)
    res_plus = sum((y_scaled-x_scaled@beta_plus)**2)
    res = x_scaled.transpose()@x_scaled@beta_temp-x_scaled.transpose()@y_scaled
    res_temp = sum((y_scaled-x_scaled@beta_temp)**2)-alpha*t_temp*sum(res**2)
    return(res_plus>res_temp)
```


```python
## ëª¨ìˆ˜ ì´ˆê¸°ê°’
beta_init = np.array([0,1,1])

## ëª¨ìˆ˜ í˜„ì¬ê°’
beta_temp = beta_init

## ìŠ¤í… ì´ˆê¸°ê°’
t_init = np.array([0.5])

## ìŠ¤í… í˜„ì¬ê°’
t_temp = t_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_BS = pd.DataFrame([[*t_init,*beta_init]],columns=["Step","beta0","beta1","beta2"])

## í•˜ì´í¼ íŒŒë¼ë¯¸í„°
alpha = 0.5
beta = 0.7

## ì´í„°ë ˆì´ì…˜ ëŒë¦¬ê¸°
for i in range(1000) : 
    while (backtracking_scaled(alpha,t_temp)):
        t_temp = t_temp * beta
    beta_temp = beta_temp - 2*t_temp*gradient_scaled(beta_temp)
    GD_result_BS = GD_result_BS.append(pd.DataFrame([[*t_temp,*beta_temp]],columns=["Step","beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.001 : break
```


```python
GD_result_BS
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Step</th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>5.000000e-01</td>
      <td>0.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>8.142068e-04</td>
      <td>0.0</td>
      <td>-0.113204</td>
      <td>0.879186</td>
    </tr>
    <tr>
      <td>2</td>
      <td>8.142068e-04</td>
      <td>0.0</td>
      <td>0.590754</td>
      <td>0.995698</td>
    </tr>
    <tr>
      <td>3</td>
      <td>8.142068e-04</td>
      <td>0.0</td>
      <td>0.144129</td>
      <td>0.896812</td>
    </tr>
    <tr>
      <td>4</td>
      <td>8.142068e-04</td>
      <td>0.0</td>
      <td>0.428400</td>
      <td>0.975238</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>7.405566e-10</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>997</td>
      <td>7.405566e-10</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>998</td>
      <td>7.405566e-10</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>999</td>
      <td>7.405566e-10</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>7.405566e-10</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 4 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_BS)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_BS,s=200,marker="o")
plt.show()
```


![png](fig/1_output_80_0.png)


#### ìŠ¤ì¼€ì¼ë§ í›„ Exact Line Search

ìœ„ì—ì„œì™€ ë§ˆì°¬ê°€ì§€ë¡œ Exact-Fixed Stepì„ ê³„ì‚°í•´ì£¼ì. 


```python
eigen = 2*np.linalg.svd(x_scaled.transpose()@x_scaled)[1]
t_fixed = 1.8/(np.min(eigen)+np.max(eigen))
t_fixed
```




    0.0008802889903057032




```python
## ì´ˆê¸°ê°’
beta_init = [0,1,1]

## í˜„ì¬ê°’
beta_temp = beta_init

## ì „ì²´ ê²°ê³¼ë¬¼ ì €ì¥ì†Œ
GD_result_CS = pd.DataFrame([beta_init],columns=["beta0","beta1","beta2"])

## ì´í„°ë ˆì´ì…˜ ëŒë¦¬ê¸°
for i in range(1000) : 
    beta_temp = beta_temp - 2*t_fixed*gradient_scaled(beta_temp)
    GD_result_CS = GD_result_CS.append(pd.DataFrame(np.array([beta_temp]),columns=["beta0","beta1","beta2"]),ignore_index=True)
    if sum(gradient(beta_temp)**2)**(1/2) < 0.001 : break
```


```python
GD_result_CS
```




<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }
    
    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>beta0</th>
      <th>beta1</th>
      <th>beta2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>0.0</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <td>1</td>
      <td>0.0</td>
      <td>-0.203553</td>
      <td>0.869381</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.0</td>
      <td>0.716992</td>
      <td>1.016173</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.0</td>
      <td>0.011059</td>
      <td>0.868236</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.0</td>
      <td>0.553808</td>
      <td>1.008583</td>
    </tr>
    <tr>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <td>996</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>997</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>998</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>999</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
    <tr>
      <td>1000</td>
      <td>0.0</td>
      <td>0.317707</td>
      <td>0.941086</td>
    </tr>
  </tbody>
</table>
<p>1001 rows Ã— 3 columns</p>
</div>




```python
plt.figure(figsize=(10,10))
sns.lineplot(x="beta1", y="beta2",
                  sort=False, data=GD_result_CS)
sns.scatterplot(x="beta1", y="beta2",data=GD_result_CS,s=200,marker="o")
plt.show()
```


![png](fig/1_output_86_0.png)


ì„¸ ê°€ì§€ ê²°ê³¼ë¬¼ì´ ëª¨ë‘ ëª©í‘œë¡œ í•˜ëŠ” ê°’ì— ì˜ ìˆ˜ë ´í•œ ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤.  

## ë¬¸ì œ D. ìœ„ì˜ ê° ê²°ê³¼ë¬¼ì˜ ì´í„°ë ˆì´ì…˜ì— ë”°ë¥¸ ì˜¤ì°¨ë¥¼ ë¹„êµí•´ì£¼ëŠ” ê·¸ë¦¼ì„ ê·¸ë ¤ë³´ì.

ë§ˆì§€ë§‰ìœ¼ë¡œ ìœ„ì˜ ì„¸ê°€ì§€ ê²°ê³¼ë¬¼ì„ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•´ë³´ì. 


```python
target_beta = [0,0.317707,0.941086]
GD_Total_Score = pd.concat([GD_result_AS.apply(lambda x : sum((target_beta-x)**2)/2 ,axis=1),GD_result_BS.apply(lambda x : sum((target_beta-x[1:4])**2)/2 ,axis=1),GD_result_CS.apply(lambda x : sum((target_beta-x)**2)/2 ,axis=1)],axis=1)
GD_Total_Score.columns = ["Fixed Step","BackTracking","Exact"]
```


```python
plt.figure(figsize=(20,10))
sns.lineplot(data=GD_Total_Score.iloc[:20], palette="tab10", linewidth=2.5)
plt.show()
```


![png](fig/1_output_91_0.png)


## ê²°ë¡ 
- ì§ì ‘ êµ¬í˜„í•´ë³¸ ê²°ê³¼ Gradient Descentë¥¼ ì˜ êµ¬í˜„í•˜ê¸° ìœ„í•´ì„œëŠ” ìŠ¤ì¼€ì¼ë§ê³¼ Backtraking Line Searchê°€ í•„ìˆ˜ì ì„ì„ ì•Œê²Œ ë˜ì—ˆë‹¤. 
- Exact Line Searchë¥¼ ìœ„í•´ì„œ Hessian Matrixë¥¼ ê°ì‹¸ëŠ” ì ì ˆí•œ ë²”ìœ„ë¥¼ ì°¾ì•„ì•¼ í•˜ëŠ”ë° ì´ë¥¼ ìœ„í•´ì„œëŠ” Spectral Decompositionì„ í•´ì¤Œìœ¼ë¡œì¨ êµ¬í•  ìˆ˜ ìˆë‹¤. 
- Exact Line Searchë¥¼ í†µí•´ì„œ êµ¬í•œ Step SizeëŠ” Gradient Descent ìˆ˜ë ´ì˜ ë¶„ê¸°ê°€ ëœë‹¤. 

## ë³´ì™„ì   
- Objective Functionì„ SSEê°€ ì•„ë‹ˆë¼ MSEë¥¼ ì“´ë‹¤ë©´ ê²°êµ­ ë¶„ì‚°ì˜ ì°¨ì´ì— ëŒ€í•œ ë³´ì •ì´ ë“¤ì–´ê°€ì„œ ìŠ¤ì¼€ì¼ë§ ì—†ì´ë„ ì–´ëŠì •ë„ ê°€ì†í™”ê°€ ë ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤. 
- ë” ë§ì€ ë³€ìˆ˜, ë” ë§ì€ ë°ì´í„°, ë” ë¨¼ ì‹œì‘ì ì—ì„œ ì–´ë–¤ ì‹ìœ¼ë¡œ ë³€í•  ì§€ì— ëŒ€í•´ì„œ ìƒê°í•´ë³´ì. 
